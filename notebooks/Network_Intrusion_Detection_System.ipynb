{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "In this step, we will explore the datasets and do a little preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling Intrusion Detection: Analysis of a Feature Selection Mechanism\n",
    "\n",
    "## Method Description\n",
    "\n",
    "### Step 1: Data preprocessing:\n",
    "All features are made numerical using one-Hot-encoding. The features are scaled to avoid features with large values that may weigh too much in the results.\n",
    "\n",
    "### Step 2: Feature Selection:\n",
    "Eliminate redundant and irrelevant data by selecting a subset of relevant features that fully represents the given problem.\n",
    "Univariate feature selection with ANOVA F-test. This analyzes each feature individually to detemine the strength of the relationship between the feature and labels. Using SecondPercentile method (sklearn.feature_selection) to select features based on percentile of the highest scores. \n",
    "When this subset is found: Recursive Feature Elimination (RFE) is applied.\n",
    "\n",
    "### Step 4: Build the model:\n",
    "Decision tree model is built.\n",
    "\n",
    "### Step 5: Prediction & Evaluation (validation):\n",
    "Using the test data to make predictions of the model.\n",
    "Multiple scores are considered such as:accuracy score, recall, f-measure, confusion matrix.\n",
    "perform a 10-fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score # for calculating accuracy of model\n",
    "from sklearn.metrics import classification_report # for generating a classification report of model\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train=pd.read_csv('../datasets/KDDTrain+.txt',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test=pd.read_csv('../datasets/KDDTest+.txt',header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample view of the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample view of the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Columns Name of Training and Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n",
    "    \"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
    "    \"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
    "    \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
    "    \"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
    "    \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
    "    \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
    "    \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
    "    \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
    "    \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"label\", \"difficulty_level\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shape of Training and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of Training Dataset:\", dataset_train.shape)\n",
    "print(\"Shape of Testing Dataset:\", dataset_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Columns Assignement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning attribute name to dataset\n",
    "dataset_train.columns = col_names\n",
    "dataset_test.columns = col_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label of training and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label distribution of Training set and testing set\n",
    "print('Label distribution Training set:')\n",
    "print(dataset_train['label'].value_counts())\n",
    "print()\n",
    "print('Label distribution Test set:')\n",
    "print(dataset_test['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-Hot-Encoding (one-of-K) is used to to transform all categorical features into binary features. Requirement for One-Hot-encoding: \"The input to this transformer should be a matrix of integers, denoting the values taken on by categorical (discrete) features. The output will be a sparse matrix where each column corresponds to one possible value of one feature. It is assumed that input features take on values in the range [0, n_values).\"\n",
    "\n",
    "Therefore the features first need to be transformed with LabelEncoder, to transform every category to a number\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Useless Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train.drop(['difficulty_level'],axis=1,inplace=True)\n",
    "dataset_test.drop(['difficulty_level'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing attack labels to their respective attack class\n",
    "\n",
    "Within the data set exists 4 different classes of attacks: Denial of Service (DoS), Probe, User to Root(U2R), and Remote to Local (R2L). The dataset contains subclasses of these attacks which we will be replacing with the main class of attacks.\n",
    "\n",
    "The sub classes of attacks are:\n",
    "\n",
    "| Dos | R2L | Probe | U2R |\n",
    "| --- | --- | --- | --- |\n",
    "| back | ftp_write | ipsweep |  buffer_overflow |\n",
    "| land | guess_passwd | mscan |  loadmodule |\n",
    "| neptune | httptunnel | nmap |  perl |\n",
    "| mailbomb | imap | portsweep |  ps |\n",
    "| pod | multihop | saint |  rootkit |\n",
    "| processtable | named | satan |  sqlattack |\n",
    "| smurf | phf |      | xterm | |\n",
    "| teardrop | sendmail |      | |\n",
    "| udpstorm | snmpgetattack |      | |\n",
    "| worm |   snmpguess |      | |\n",
    "|       | spy |      | |\n",
    "|       | warezclient |      | |\n",
    "|       | warezmaster |      | |\n",
    "|       | xlock |      | |\n",
    "|       | xsnoop |      | |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_label_of(df):\n",
    "  df.label.replace(['apache2','back','land','neptune','mailbomb','pod','processtable','smurf','teardrop','udpstorm','worm'],'Dos',inplace=True)\n",
    "  df.label.replace(['ftp_write','guess_passwd','httptunnel','imap','multihop','named','phf','sendmail',\n",
    "       'snmpgetattack','snmpguess','spy','warezclient','warezmaster','xlock','xsnoop'],'R2L',inplace=True)\n",
    "  df.label.replace(['ipsweep','mscan','nmap','portsweep','saint','satan'],'Probe',inplace=True)\n",
    "  df.label.replace(['buffer_overflow','loadmodule','perl','ps','rootkit','sqlattack','xterm'],'U2R',inplace=True)\n",
    "\n",
    "\n",
    "change_label_of(dataset_train)\n",
    "change_label_of(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label distribution of Training set and testing set after relabeling\n",
    "print('Label distribution Training set:')\n",
    "print(dataset_train['label'].value_counts())\n",
    "print()\n",
    "print('Label distribution Test set:')\n",
    "print(dataset_test['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore categorical features\n",
    "print('Training set:')\n",
    "for col_name in dataset_train.columns:\n",
    "    if dataset_train[col_name].dtypes == 'object' :\n",
    "        unique_cat = len(dataset_train[col_name].unique())\n",
    "        print(\"Feature '{col_name}' has {unique_cat} categories\".format(col_name=col_name, unique_cat=unique_cat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set\n",
    "print('Test set:')\n",
    "for col_name in dataset_test.columns:\n",
    "    if dataset_test[col_name].dtypes == 'object' :\n",
    "        unique_cat = len(dataset_test[col_name].unique())\n",
    "        print(\"Feature '{col_name}' has {unique_cat} categories\".format(col_name=col_name, unique_cat=unique_cat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding categorical features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert categorical features into a 2D numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns=['protocol_type', 'service', 'flag']\n",
    "label = [\"label\"]\n",
    "\n",
    "# seperate categorical data from non-categorical data and labels\n",
    "categorical_train_data = dataset_train[categorical_columns]\n",
    "non_categorical_train_data = dataset_train.drop(categorical_columns+label, axis=1)\n",
    "\n",
    "categorical_test_data = dataset_test[categorical_columns]\n",
    "non_categorical_test_data = dataset_test.drop(categorical_columns+label, axis=1)\n",
    "\n",
    "# Separate label from dataset\n",
    "label_train_data = dataset_train['label']\n",
    "label_test_data = dataset_test['label']\n",
    "\n",
    "# visualize categorical data\n",
    "categorical_train_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# convert categorical data to numeric using one-hot encoding\n",
    "categorical_train_data_encoded = pd.get_dummies(categorical_train_data)\n",
    "categorical_test_data_encoded = pd.get_dummies(categorical_test_data)\n",
    "\n",
    "# visualize encoded categorical data\n",
    "categorical_train_data_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Normalization\n",
    "We will use MinMaxScaler to normalize the non-categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "# Function to normalize the dataset\n",
    "def normalization(df):\n",
    "  for i in df.columns:\n",
    "    arr = df[i]\n",
    "    arr = np.array(arr)\n",
    "    df[i] = scaler.fit_transform(arr.reshape(len(arr),1))\n",
    "  return df\n",
    "\n",
    "\n",
    "# Normalize the training set\n",
    "non_categorical_train_data = normalization(non_categorical_train_data)\n",
    "\n",
    "# Normalize the test set\n",
    "non_categorical_test_data = normalization(non_categorical_test_data)\n",
    "\n",
    "non_categorical_train_data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection with Principal Component Analysis (PCA)\n",
    "\n",
    "We will now perform PCA on the non-catergorical dataset to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  feature selection using pca\n",
    "from sklearn.decomposition import PCA\n",
    "# We choose the minimum number of principal components \n",
    "# such that 95% of the variance is retained.\n",
    "pca = PCA(.95)\n",
    "\n",
    "pca.fit(non_categorical_train_data)\n",
    "\n",
    "non_categorical_train_data_pca = pca.transform(non_categorical_train_data)\n",
    "\n",
    "non_categorical_test_data_pca = pca.transform(non_categorical_test_data)\n",
    "\n",
    "non_categorical_train_data_pca = pd.DataFrame(data = non_categorical_train_data_pca,\n",
    "                                                columns = [f\"component_{i}\" for i in range(1, pca.n_components_+1)])\n",
    "\n",
    "non_categorical_test_data_pca = pd.DataFrame(data = non_categorical_test_data_pca,\n",
    "                                                columns = [f\"component_{i}\" for i in range(1, pca.n_components_+1)])\n",
    "\n",
    "non_categorical_test_data_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of components selected\n",
    "print(\"number of features selected: \", pca.n_components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding the Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column in the label dataset with the ecoded labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "encoded_train_labels = label_encoder.fit_transform(label_train_data)\n",
    "encoded_test_labels = label_encoder.fit_transform(label_test_data)\n",
    "\n",
    "label_classes = label_encoder.classes_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join encoded categorical dataframe with the non-categorical dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_test_data_encoded\n",
    "categorical_train_data_encoded\n",
    "\n",
    "non_categorical_test_data_pca\n",
    "non_categorical_train_data_pca\n",
    "\n",
    "# concatenate the categorical and non-categorical data and labels\n",
    "train_data = pd.concat([categorical_train_data_encoded, non_categorical_train_data_pca], axis=1)\n",
    "test_data = pd.concat([categorical_test_data_encoded, non_categorical_test_data_pca], axis=1)\n",
    "\n",
    "train_data[\"target\"] = encoded_train_labels\n",
    "test_data[\"target\"] = encoded_test_labels\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.pie(train_data.target.value_counts(),labels=train_data.target.unique(),autopct='%0.2f%%')\n",
    "plt.title('Pie chart distribution of labels')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the models\n",
    "\n",
    "### Here, we will build 3 models:\n",
    "### 1. Linear SVM\n",
    "### 2. K-Nearest Neighbors\n",
    "### 3. AdaBoost Ensemble Model with Linear SVM and K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Training and Testing variables\n",
    "\n",
    "x_train = train_data.drop(['target'],axis=1).to_numpy()\n",
    "y_train = train_data.target\n",
    "x_test = test_data.drop(['target'],axis=1).to_numpy()\n",
    "y_test = test_data.target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel='linear',gamma='auto')\n",
    "svm.fit(x_train,y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_pred = svm.predict(x_test) \n",
    "svm_ac=accuracy_score(y_test, svm_pred)*100  \n",
    "print(\"The Accuracy of SVM-Classifier is: \", svm_ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM classification report\n",
    "classification_report(y_test, svm_pred,target_names=label_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-nearest-neighbor Classifier (Multi-class Classification)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn=KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(x_train,y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pred=knn.predict(x_test)  \n",
    "knn_ac=accuracy_score(y_test, knn_pred)*100  \n",
    "\n",
    "print(\"The Accuracy of the KNN-Classifier is: \", knn_ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report\n",
    "classification_report(y_test, knn_pred,target_names=label_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost Ensemble Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5851d4060600120c75427bac5f0fc671e33d7f75731658bd704817a1b0f623c2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('Intrusion_Detection_System_Analysis-9ZPrhidj': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}